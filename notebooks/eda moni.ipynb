{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze Layer\n",
    "\n",
    "In this notebook ETL processes of data in bronze layer are develop and tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerias python\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar modulos propios en notebook\n",
    "project_root = Path(os.getcwd()).parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db.connection import engine_setting, engine_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rootFolder = Path.cwd().parent\n",
    "\n",
    "excel_path_1_bonos_trab =  rootFolder / \"data\" / \"U532-2024-11-05.xls\"\n",
    "excel_path_2_taller_orden_rep =  rootFolder / \"data\" / \"U533-2024-11-05.xls\"\n",
    "excel_path_3_bonos_pres =  rootFolder / \"data\" / \"U551-2024-11-05.xls\"\n",
    "excel_path_4_stock =  rootFolder / \"data\" / \"U552-2024-11-05.xls\"\n",
    "excel_path_5_compras =  rootFolder / \"data\" / \"U553-2024-11-05.xls\"\n",
    "excel_path_6_mostrador =  rootFolder / \"data\" / \"U554-2024-11-05.xls\"\n",
    "excel_path_7_invertidas =  rootFolder / \"data\" / \"U555-2024-11-05.xls\"\n",
    "excel_path_8_taller_orden_venta =  rootFolder / \"data\" / \"U560-2024-11-05.xls\"\n",
    "excel_path_9_cliente =  rootFolder / \"data\" / \"U6301303-conta-2024-11-06.xls\"\n",
    "excel_path_10_articulo =  rootFolder / \"data\" / \"U6301303-taller-2024-11-05.xls\"\n",
    "excel_path_11_empresas =  rootFolder / \"data\" / \"U6311303-conta-2024-11-06.xls\"\n",
    "excel_path_12_talleres =  rootFolder / \"data\" / \"U6311303-taller-2024-11-05.xls\"\n",
    "excel_path_13_almacenes =  rootFolder / \"data\" / \"U6321303-2024-11-05.xls\"\n",
    "excel_path_14_operarios =  rootFolder / \"data\" / \"U6331303-2024-11-05.xls\"\n",
    "excel_path_15_vehiculo =  rootFolder / \"data\" / \"U6341303-2024-11-05.xls\"\n",
    "excel_path_16_tipos_horas =  rootFolder / \"data\" / \"ULSTTHPT-2024-11-05.xls\"\n",
    "excel_path_17_tipos_venta =  rootFolder / \"data\" / \"ULSTTVPT-2024-11-05.xls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** file size (51826) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "rootFolder = Path.cwd().parent\n",
    "\n",
    "clientes_a = rootFolder/ 'data' / 'U6301303_A_20241202.xls'\n",
    "\n",
    "df_clientes_a = pd.read_excel(clientes_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo Excel con la ruta completa\n",
    "df_bonos_trab = pd.read_excel(excel_path_1_bonos_trab)\n",
    "df_taller_orden_rep = pd.read_excel(excel_path_2_taller_orden_rep)\n",
    "df_bono_pres = pd.read_excel(excel_path_3_bonos_pres)\n",
    "df_stock = pd.read_excel(excel_path_4_stock)\n",
    "df_compras = pd.read_excel(excel_path_5_compras)\n",
    "df_mostrador = pd.read_excel(excel_path_6_mostrador)\n",
    "df_invertidas = pd.read_excel(excel_path_7_invertidas).ffill()\n",
    "df_taller_orden_venta = pd.read_excel(excel_path_8_taller_orden_venta)\n",
    "df_cliente = pd.read_excel(excel_path_9_cliente)\n",
    "df_articulos = pd.read_excel(excel_path_10_articulo)\n",
    "df_empresas = pd.read_excel(excel_path_11_empresas)\n",
    "df_talleres = pd.read_excel(excel_path_12_talleres)\n",
    "df_almacenes = pd.read_excel(excel_path_13_almacenes)\n",
    "df_operarios = pd.read_excel(excel_path_14_operarios)\n",
    "df_vehiculos = pd.read_excel(excel_path_15_vehiculo)\n",
    "df_tipos_horas = pd.read_excel(excel_path_16_tipos_horas)\n",
    "df_tipos_venta = pd.read_excel(excel_path_17_tipos_venta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexión con capa bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = Connection to schema ddbb =\n",
    "\n",
    "# Define database type and port \n",
    "db_type = 'postgresql' \n",
    "port = 5432\n",
    "\n",
    "# Get schema name acording to layer name\n",
    "db_schema = 'bronze'\n",
    "\n",
    "# Create engine for the specific schema\n",
    "engine = engine_setting(db_type=db_type, db_port=port, db_schema=db_schema)\n",
    "\n",
    "# Establish the connection\n",
    "conn = engine_connection(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de limpieza y transformación de datos.\n",
    "\n",
    "- Eliminar columnas completamente vacias.\n",
    "- Eliminar espacios vacios en cada registro.\n",
    "- Verificar las columns que podrían ser números enteros 11 en lugar de 11.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_columns_to_str (df):\n",
    "    '''\n",
    "    This function cleans and transforms a DataFrame `df` with the following steps:\n",
    "    \n",
    "    1. Drops rows where all columns contain NaN values.\n",
    "    2. Filters out rows that contain the text '***' in any text column.\n",
    "    3. Converts float-type columns to integer type (Int64) if all non-NaN values in the column are integers.\n",
    "    4. Converts all columns to strings (`str`), replacing NaN values with empty strings and removing leading and trailing whitespace.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The DataFrame to be transformed and cleaned.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned and transformed DataFrame.\n",
    "    '''\n",
    "    # Drop rows where all columns are NaN\n",
    "    df.dropna(how='all', inplace=True)\n",
    "\n",
    "    # Apply the filter to remove rows that contain '***' in any text column\n",
    "    for column in df.select_dtypes(include='object').columns:\n",
    "        df = df[~df[column].astype(str).str.contains(r'\\*\\*\\*', na=False)]\n",
    "    \n",
    "    # cheach each column of float type to check if it can be integer\n",
    "    for column in df.select_dtypes(include=[\"float\"]).columns:\n",
    "        # verifies if each non nan values of the column are integers\n",
    "        if df[column].dropna().apply(float.is_integer).all():\n",
    "            # Converts the column to int, keeping NaN values\n",
    "            df[column] = df[column].astype(\"Int64\")\n",
    "\n",
    "    for column in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    # Convert values that can be interpreted as floats to int, keep NaN or non-numeric values\n",
    "        df[column] = df[column].apply(lambda x: str(x) if pd.isna(x) else int(x) if isinstance(x, float) else x)\n",
    "            \n",
    "\n",
    "    \n",
    "    for column in df.columns:\n",
    "        # df[column] = df[column].astype(str)\n",
    "        # df[column] = df[column].replace('nan', '')\n",
    "        # df[column] = df[column].replace('<NA>', '')\n",
    "        # df[column] = df[column].replace('NaT', '')\n",
    "        # df[column] = df[column].str.strip()\n",
    "\n",
    "        df[column] = df[column].astype(str)\n",
    "        df[column] = df[column].replace('nan', None)\n",
    "        df[column] = df[column].replace('<NA>', None)\n",
    "        df[column] = df[column].replace('NaT', None)\n",
    "        df[column] = df[column].str.strip()\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_empresas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_empresas \u001b[38;5;241m=\u001b[39m all_columns_to_str(\u001b[43mdf_empresas\u001b[49m)\n\u001b[0;32m      2\u001b[0m df_almacenes \u001b[38;5;241m=\u001b[39m all_columns_to_str(df_almacenes)\n\u001b[0;32m      3\u001b[0m df_operarios \u001b[38;5;241m=\u001b[39m all_columns_to_str(df_operarios)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_empresas' is not defined"
     ]
    }
   ],
   "source": [
    "df_empresas = all_columns_to_str(df_empresas)\n",
    "df_almacenes = all_columns_to_str(df_almacenes)\n",
    "df_operarios = all_columns_to_str(df_operarios)\n",
    "df_vehiculos = all_columns_to_str(df_vehiculos)\n",
    "df_tipos_horas = all_columns_to_str(df_tipos_horas)\n",
    "df_tipos_venta = all_columns_to_str(df_tipos_venta)\n",
    "df_cliente = all_columns_to_str(df_cliente)\n",
    "df_articulos = all_columns_to_str(df_articulos)\n",
    "df_talleres = all_columns_to_str(df_talleres)\n",
    "\n",
    "df_bonos_trab = all_columns_to_str(df_bonos_trab) \n",
    "df_taller_orden_rep = all_columns_to_str(df_taller_orden_rep) \n",
    "df_bono_pres = all_columns_to_str(df_bono_pres)\n",
    "df_stock = all_columns_to_str(df_stock)\n",
    "df_compras = all_columns_to_str(df_compras)\n",
    "df_mostrador = all_columns_to_str(df_mostrador)\n",
    "df_invertidas = all_columns_to_str(df_invertidas)\n",
    "df_taller_orden_venta = all_columns_to_str(df_taller_orden_venta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns_to_str(df_clientes_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processes.bronze.extract import incremental_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from processes.bronze.extract import incrementales_contabilidad, prefijos_bronze, list_files_in_most_recent_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas_recientes_contabilidad, ficheros = list_files_in_most_recent_date(incrementales_contabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** file size (52481) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (14082) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (181436) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (81569) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (12092113) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (2791515) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (1032552) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** file size (792390) not 512 + multiple of sector size (512)\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n"
     ]
    }
   ],
   "source": [
    "df_cliente = incremental_data(incrementales_contabilidad, mas_recientes_contabilidad, prefijos_bronze[\"clientes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Codigo cuenta</th>\n",
       "      <th>Cat</th>\n",
       "      <th>Nombre cliente</th>\n",
       "      <th>DNI/CIF</th>\n",
       "      <th>Dirección</th>\n",
       "      <th>C.P.</th>\n",
       "      <th>Localidad</th>\n",
       "      <th>E-mail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5260</td>\n",
       "      <td>A</td>\n",
       "      <td>HERRERA BALBONTIN BONIFACIO</td>\n",
       "      <td>13740979C</td>\n",
       "      <td>Bº IGLESIA, 19-A CHALET 5</td>\n",
       "      <td>39192</td>\n",
       "      <td>SAN MAMES DE MERUELO (CANTABRIA)</td>\n",
       "      <td>boheba@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12222</td>\n",
       "      <td>A</td>\n",
       "      <td>BARCENA FERNANDEZ CESAR</td>\n",
       "      <td>13736278B</td>\n",
       "      <td>LA VENTILLA 41</td>\n",
       "      <td>39710</td>\n",
       "      <td>SOLARES (CANTABRIA)</td>\n",
       "      <td>cbarcena@lodiscan.es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14247</td>\n",
       "      <td>A</td>\n",
       "      <td>PAZOS DEL RIO,FERNANDO</td>\n",
       "      <td>30557396X</td>\n",
       "      <td>ALTE M VALLECILLA 4 4 CT I</td>\n",
       "      <td>48920</td>\n",
       "      <td>PPORTUGALETE (VIZCAYA)</td>\n",
       "      <td>fdopdelrio@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15225</td>\n",
       "      <td>A</td>\n",
       "      <td>FERREIRA AGUIAR ANTONIO JOSE</td>\n",
       "      <td>72582222H</td>\n",
       "      <td>ERREBALBURU, 4 4º D</td>\n",
       "      <td>48260</td>\n",
       "      <td>ERMUA-BIZKAIA</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16298</td>\n",
       "      <td>A</td>\n",
       "      <td>PEÑA SAN EMETERIO VICTORINO</td>\n",
       "      <td>13772496G</td>\n",
       "      <td>RESIDENCIA EL SOL, 2 - CHALET. Nº 8 Bº SOMAVILLA</td>\n",
       "      <td>39766</td>\n",
       "      <td>SAN PANTALEON DE ARAS (CANTABRIA)</td>\n",
       "      <td>facturacionvpena@gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79463</th>\n",
       "      <td>X5006</td>\n",
       "      <td>T</td>\n",
       "      <td>JOSE RAMON SECO</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79464</th>\n",
       "      <td>X5007</td>\n",
       "      <td>T</td>\n",
       "      <td>YOLANDA ALONSO</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79465</th>\n",
       "      <td>X5008</td>\n",
       "      <td>T</td>\n",
       "      <td>LUIS CARLOS</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79466</th>\n",
       "      <td>X5009</td>\n",
       "      <td>T</td>\n",
       "      <td>JOSE BATS OLAGO</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79467</th>\n",
       "      <td>X5010</td>\n",
       "      <td>T</td>\n",
       "      <td>LORENZO GONZALEZ PINTO</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79378 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Codigo cuenta Cat                Nombre cliente    DNI/CIF  \\\n",
       "0              5260   A   HERRERA BALBONTIN BONIFACIO  13740979C   \n",
       "1             12222   A       BARCENA FERNANDEZ CESAR  13736278B   \n",
       "2             14247   A        PAZOS DEL RIO,FERNANDO  30557396X   \n",
       "3             15225   A  FERREIRA AGUIAR ANTONIO JOSE  72582222H   \n",
       "4             16298   A   PEÑA SAN EMETERIO VICTORINO  13772496G   \n",
       "...             ...  ..                           ...        ...   \n",
       "79463         X5006   T               JOSE RAMON SECO       None   \n",
       "79464         X5007   T                YOLANDA ALONSO       None   \n",
       "79465         X5008   T                   LUIS CARLOS       None   \n",
       "79466         X5009   T               JOSE BATS OLAGO       None   \n",
       "79467         X5010   T        LORENZO GONZALEZ PINTO       None   \n",
       "\n",
       "                                              Dirección   C.P.  \\\n",
       "0                             Bº IGLESIA, 19-A CHALET 5  39192   \n",
       "1                                        LA VENTILLA 41  39710   \n",
       "2                            ALTE M VALLECILLA 4 4 CT I  48920   \n",
       "3                                   ERREBALBURU, 4 4º D  48260   \n",
       "4      RESIDENCIA EL SOL, 2 - CHALET. Nº 8 Bº SOMAVILLA  39766   \n",
       "...                                                 ...    ...   \n",
       "79463                                              None   None   \n",
       "79464                                              None   None   \n",
       "79465                                              None   None   \n",
       "79466                                              None   None   \n",
       "79467                                              None   None   \n",
       "\n",
       "                               Localidad                      E-mail  \n",
       "0       SAN MAMES DE MERUELO (CANTABRIA)            boheba@gmail.com  \n",
       "1                    SOLARES (CANTABRIA)        cbarcena@lodiscan.es  \n",
       "2                 PPORTUGALETE (VIZCAYA)        fdopdelrio@gmail.com  \n",
       "3                          ERMUA-BIZKAIA                        None  \n",
       "4      SAN PANTALEON DE ARAS (CANTABRIA)  facturacionvpena@gmail.com  \n",
       "...                                  ...                         ...  \n",
       "79463                               None                        None  \n",
       "79464                               None                        None  \n",
       "79465                               None                        None  \n",
       "79466                               None                        None  \n",
       "79467                               None                        None  \n",
       "\n",
       "[79378 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "all_columns_to_str(df_cliente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         5260.0\n",
       "1        12222.0\n",
       "2        14247.0\n",
       "3        15225.0\n",
       "4        16298.0\n",
       "          ...   \n",
       "79463      X5006\n",
       "79464      X5007\n",
       "79465      X5008\n",
       "79466      X5009\n",
       "79467      X5010\n",
       "Name: Codigo cuenta, Length: 79468, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cliente['Codigo cuenta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         5260\n",
       "1        12222\n",
       "2        14247\n",
       "3        15225\n",
       "4        16298\n",
       "         ...  \n",
       "79463    X5006\n",
       "79464    X5007\n",
       "79465    X5008\n",
       "79466    X5009\n",
       "79467    X5010\n",
       "Name: Codigo cuenta, Length: 79393, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cliente['Codigo cuenta'].apply(lambda x: str(x) if pd.isna(x) else int(x) if isinstance(x, float) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5260\n",
      "12222\n",
      "14247\n",
      "15225\n",
      "16298\n"
     ]
    }
   ],
   "source": [
    "for value in df_cliente['Codigo cuenta'].head(5):\n",
    "    try:\n",
    "        print(str(int(value)))\n",
    "\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cliente['Codigo cuenta'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data upload - Master tables\n",
    "\n",
    "### Update new data\n",
    "\n",
    "**Step 1**: Get existing values of the reference column in ddbb table.\n",
    "\n",
    "**Step 2**: Compare dataframe values with ddbb existing values.\n",
    "\n",
    "**Step 3**: If there are new values, values are going to be written in th bbdd.\n",
    "\n",
    "**Considerations**\n",
    "- All handle values need to be str type.\n",
    "- Clean white spaces for all values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One function \n",
    "\n",
    "def upload_new_data_master (table_name, df, key_column, engine):\n",
    "    '''\n",
    "    Process new values: \n",
    "    - Get existing values from the database.\n",
    "    - Identify new values in the dataframe.\n",
    "    - Write new values to the database table.\n",
    "\n",
    "    input: \n",
    "        table_name (str): name of the table in the database.\n",
    "        df (DataFrame): dataframe to be compared with existing values.\n",
    "        key_column (str): key column name in the dataframe and table database to compare.\n",
    "        engine (SQLAlchemy engine): database connection.\n",
    "\n",
    "    output:\n",
    "        None\n",
    "    '''\n",
    "    # Get existing values from the database\n",
    "    query = f'SELECT \"{key_column}\" FROM \"{table_name}\"'\n",
    "    existing_values = pd.read_sql(query, con=engine)[key_column].tolist()\n",
    "\n",
    "    # Get new data by comparing dataframe with existing values\n",
    "    df_new_data = df[~df[key_column].isin(existing_values)]\n",
    "\n",
    "    # Write new values to the database\n",
    "    if not df_new_data.empty:\n",
    "        df_new_data.to_sql(table_name, con=engine, if_exists='append', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update modified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data form database\n",
    "\n",
    "def get_database_table (table_name):\n",
    "\n",
    "    query = f'SELECT * FROM \"{table_name}\"'\n",
    "    db_table = pd.read_sql(query, con=engine)\n",
    "\n",
    "    return (db_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update modified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulación \n",
    "\n",
    "df_empresas_mod = df_empresas.copy()\n",
    "df_empresas_mod.loc[df_empresas_mod['Codig'] == '13', 'Nombre'] = 'Nombre Empresa Modificado'\n",
    "df_empresas_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_empresas_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = df_empresas_mod.merge(\n",
    "        df_empresas, on='Codig',\n",
    "        suffixes=('_new', '_db')\n",
    "    )\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and make sure both DataFrames have the same column names and index\n",
    "new_merge_data = merge_data.filter(regex='_new$').sort_index(axis=1)\n",
    "db_data = merge_data.filter(regex='_db$').sort_index(axis=1)\n",
    "\n",
    "# Otherwise column names are adjusted\n",
    "new_merge_data.columns = new_merge_data.columns.str.replace('_new$', '', regex=True)\n",
    "db_data.columns = db_data.columns.str.replace('_db$', '', regex=True)\n",
    "\n",
    "# Compare rows where there are differences\n",
    "modified_data = merge_data[(new_merge_data != db_data).any(axis=1)]\n",
    "\n",
    "modified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data update - Fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "def upload_new_data_fact (table_name, df, key_column, date_column, engine):\n",
    "    '''\n",
    "    Process new values: \n",
    "    - Get existing values from the database.\n",
    "    - Identify new values in the dataframe.\n",
    "    - Write new values to the database table.\n",
    "\n",
    "    input: \n",
    "        table_name (str): name of the table in the database.\n",
    "        df (DataFrame): dataframe to be compared with existing values.\n",
    "        key_column (str): key column name in the dataframe and table database to compare.\n",
    "        date_column (str): date column name in the dataframe and table database to compare.\n",
    "        engine (SQLAlchemy engine): database connection.\n",
    "\n",
    "    output:\n",
    "        None\n",
    "    '''\n",
    "    # get date range\n",
    "    current_date = date.today()\n",
    "    min_date = current_date - timedelta(days=120)\n",
    "\n",
    "    current_date = current_date.strftime('%Y-%m-%d')\n",
    "    min_date = min_date.strftime('%Y-%m-%d') # pd.to_datetime(df[date_column], format='%Y-%m-%d', errors='coerce').min().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Get existing values from the database\n",
    "    query = f'''\n",
    "        SELECT \"{key_column}\", \"{date_column}\"\n",
    "        FROM \"{table_name}\" \n",
    "        WHERE \"{date_column}\" BETWEEN '{min_date}' AND '{current_date}'\n",
    "        '''\n",
    "    existing_values = pd.read_sql(query, con=engine)\n",
    "\n",
    "    # Get new data by comparing dataframe with existing values\n",
    "\n",
    "    # Generate combined keys for comparison to identify new data\n",
    "    df = df.assign(\n",
    "        combined_key=df[key_column].astype(str) + \"_\" + df[date_column].astype(str)\n",
    "    )\n",
    "    existing_values = existing_values.assign(\n",
    "        combined_key=existing_values[key_column].astype(str) + \"_\" + existing_values[date_column].astype(str)\n",
    "    )\n",
    "\n",
    "    # Filter rows in df that are not in existing_values\n",
    "    df_new_data = df[~df['combined_key'].isin(existing_values['combined_key'])]\n",
    "\n",
    "    # Drop the temporary 'combined_key' column before saving new values\n",
    "    df_new_data = df_new_data.drop(columns='combined_key')\n",
    "    existing_values = existing_values.drop(columns='combined_key')\n",
    "\n",
    "    # Write new values to the database\n",
    "    if not df_new_data.empty:\n",
    "        df_new_data.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "\n",
    "    # Optional: return df_new_data if needed for further processing or testing\n",
    "    # return df_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_taller_orden_rep['REF.OR'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taller_orden_rep['REF.OR'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taller_orden_rep[df_taller_orden_rep['REF.OR'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taller_orden_rep.loc[1180: 1190, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert existing data sample\n",
    "df_taller_orden_rep_sample =  df_taller_orden_rep.loc[:15, :]\n",
    "upload_new_data_fact ('Taller OrdenReparacion U533', df_taller_orden_rep_sample, 'REF.OR', 'Fec.apertu', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload new data sample\n",
    "df_taller_orden_rep_new_data = df_taller_orden_rep.loc[:25, :]\n",
    "upload_new_data_fact ('Taller OrdenReparacion U533', df_taller_orden_rep_new_data, 'REF.OR', 'Fec.apertu', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla Ordenes Venta Mostrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mostrador_sample = df_mostrador.loc[:15, :]\n",
    "df_mostrador_new_data = df_mostrador.loc[:25, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact('Taller OrdenVentaMostrador U554', df_mostrador_sample, 'Refer.', 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact('Taller OrdenVentaMostrador U554', df_mostrador_new_data, 'Refer.', 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla Ordenes Venta taller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_taller_ord_venta_sample = df_taller_orden_venta.loc[:14, :]\n",
    "df_taller_ord_venta_new_data = df_taller_orden_venta.loc[:24, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact('Taller OrdenVentaTaller U560', df_taller_ord_venta_sample, 'Refer.', 'Fec.sali', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact('Taller OrdenVentaTaller U560', df_taller_ord_venta_new_data, 'Refer.', 'Fec.sali', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla Stock - No tiene key column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_sample = df_stock.loc[:14, :]\n",
    "df_stock_new_data = df_stock.loc[:24, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact('Taller OrdenVentaTaller U560', df_taller_ord_venta_sample, 'Refer.', 'Fec.sali', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock['Artículo'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock[df_stock['Artículo'] == '3/CA679']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación de Tabla Compras - agregar otra key_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compras_sample = df_compras.loc[:14, :]\n",
    "df_compras_new_data = df_compras.loc[:24, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact('Compras U553', df_compras_sample, 'Referenci', 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact('Compras U553', df_compras_new_data, 'Referenci', 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación de Tabla Bonos Presencia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_new_data_fact_mult (table_name, df, key_columns, date_column, engine):\n",
    "    '''\n",
    "    Process new values: \n",
    "    - Get existing values from the database.\n",
    "    - Identify new values in the dataframe.\n",
    "    - Write new values to the database table.\n",
    "\n",
    "    input: \n",
    "        table_name (str): name of the table in the database.\n",
    "        df (DataFrame): dataframe to be compared with existing values.\n",
    "        key_columns (list): list ok key column names in the dataframe and table database to compare.\n",
    "        date_colum (str): date column name in the dataframe and table database to compare.\n",
    "        engine (SQLAlchemy engine): database connection.\n",
    "\n",
    "    output:\n",
    "        None\n",
    "    '''\n",
    "    # get date range\n",
    "    current_date = date.today()\n",
    "    min_date = current_date - timedelta(days=120)\n",
    "\n",
    "    current_date = current_date.strftime('%Y-%m-%d')\n",
    "    min_date = min_date.strftime('%Y-%m-%d') # pd.to_datetime(df[date_column], format='%Y-%m-%d', errors='coerce').min().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Get existing values from the database\n",
    "    query = f'''\n",
    "        SELECT \"{key_columns[0]}\", \"{key_columns[1]}\", \"{date_column}\"\n",
    "        FROM \"{table_name}\" \n",
    "        WHERE \"{date_column}\" BETWEEN '{min_date}' AND '{current_date}'\n",
    "        '''\n",
    "    existing_values = pd.read_sql(query, con=engine)#[key_column].tolist()\n",
    "\n",
    "    # Get new data by comparing dataframe with existing values\n",
    "    # Generate combined keys for comparison to identify new data\n",
    "    #df.loc[:, 'combined_key'] = df[key_columns[0]].astype(str) + \"_\" + df[key_columns[1]].astype(str) + \"_\" + df[date_column].astype(str)\n",
    "    #existing_values.loc[:, 'combined_key'] = existing_values[key_columns[0]].astype(str) + \"_\" + existing_values[key_columns[1]].astype(str) + \"_\" + existing_values[date_column].astype(str)\n",
    "\n",
    "    # Crear 'combined_key' en df\n",
    "    df = df.assign(\n",
    "        combined_key=df[key_columns[0]].astype(str) + \"_\" + \n",
    "                    df[key_columns[1]].astype(str) + \"_\" + \n",
    "                    df[date_column].astype(str)\n",
    "    )\n",
    "\n",
    "    # Crear 'combined_key' en existing_values\n",
    "    existing_values = existing_values.assign(\n",
    "        combined_key=existing_values[key_columns[0]].astype(str) + \"_\" + \n",
    "                    existing_values[key_columns[1]].astype(str) + \"_\" + \n",
    "                    existing_values[date_column].astype(str)\n",
    "    )\n",
    "\n",
    "    # Filter rows in df that are not in existing_values\n",
    "    df_new_data = df[~df['combined_key'].isin(existing_values['combined_key'])]\n",
    "\n",
    "    # Drop the temporary 'combined_key' column before saving new values\n",
    "    df_new_data = df_new_data.drop(columns='combined_key')\n",
    "    existing_values = existing_values.drop(columns='combined_key')\n",
    "\n",
    "    # Write new values to the database\n",
    "    if not df_new_data.empty:\n",
    "        df_new_data = df_new_data.copy().sort_values(date_column, ascending=True, inplace=True)\n",
    "        df_new_data.to_sql(table_name, con=engine, if_exists='append', index=False)\n",
    "\n",
    "    # Optional: return df_new_data if needed for further processing or testing\n",
    "    # return df_new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla Bonos Presencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bono_pres_sample = df_bono_pres.loc[:15, :]\n",
    "df_bono_pres_new_data = df_bono_pres.loc[:24, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('U551_Presencia', df_bono_pres_sample, ['Operario', 'ENTRADA PRES'], 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('U551_Presencia', df_bono_pres_new_data, ['Operario', 'ENTRADA PRES'], 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bono_pres_sample.sort_values('Fecha', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bono_pres_new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla Bonos Trabajadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bonos_trab_sample = df_bonos_trab.loc[:15, :]\n",
    "df_bonos_trab_new_data = df_bonos_trab.loc[:25, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('BonosTrabajadas U532', df_bonos_trab_sample, ['Operario', 'Entra'], 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('BonosTrabajadas U532', df_bonos_trab_new_data, ['Operario', 'Entra'], 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla Compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compras_sample = df_compras.loc[:15, :]\n",
    "df_compras_new_data = df_compras.loc[:25, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('Compras U553', df_compras_sample, ['Referenci', 'Artículo'], 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('Compras U553', df_compras_new_data, ['Referenci', 'Artículo'], 'Fecha', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla invertidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invertidas_sample = df_invertidas.loc[:15, :]\n",
    "df_invertidas_new_data = df_invertidas.loc[:25, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('Invertidas U555', df_invertidas_sample, ['REF.OR', 'Operario'], 'Fec.apertu', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('Invertidas U555', df_invertidas_new_data, ['REF.OR', 'Operario'], 'Fec.apertu', engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validación Tabla Stock "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stock_sample = df_stock.loc[:15, :]\n",
    "df_stock_new_data = df_stock.loc[:25, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('Stock U552', df_stock_sample, ['Alma', 'Ref.articulo'], 'F.ul.ent', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_new_data_fact_mult('Stock U552', df_stock_new_data, ['Alma', 'Ref.articulo'], 'F.ul.ent', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conn:\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create historic and new data tests dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tabla bonos presencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bono_pres.sort_values('Fecha', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proporción del tamaño de la muestra (ejemplo: 70% para df1, 30% para df2)\n",
    "tamano_muestra_1 = int(len(df_bono_pres) * 0.7)\n",
    "\n",
    "# División del DataFrame\n",
    "df1 = df_bono_pres.iloc[:tamano_muestra_1]  # Primera parte\n",
    "df2 = df_bono_pres.iloc[-tamano_muestra_1:]  # Segunda parte (con intersección)\n",
    "\n",
    "# Uniendo algunos datos repetidos (puede ajustarse al criterio)\n",
    "df_comun = df_bono_pres.iloc[tamano_muestra_1//2:tamano_muestra_1]\n",
    "\n",
    "# Agregar datos comunes\n",
    "df1 = pd.concat([df1, df_comun]).drop_duplicates().reset_index(drop=True)\n",
    "df2 = pd.concat([df2, df_comun]).drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# print(\"Primera muestra (df1):\")\n",
    "# print(df1)\n",
    "\n",
    "# print(\"\\nSegunda muestra (df2):\")\n",
    "# print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
